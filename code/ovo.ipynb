{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = np.loadtxt('../support/zipcombo.dat')\n",
    "# data = np.loadtxt('../support/dtrain123.dat')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(9298, 257)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def split_80_20(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits 80% train 20% test\n",
    "\n",
    "    :param data: sequence.\n",
    "    :return: train_data, test_data: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]\n",
    "    train_size = int(n*0.8)\n",
    "    return data[:train_size], data[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def split_X_y(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits the data into datapoints and labels, X_train matrix and y_train;\n",
    "    :param data: np.ndarray\n",
    "    :return: X_train, y_train: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, 1:], data[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def shuffle_split(data):\n",
    "    # np.random.seed(seed)\n",
    "    shuffled = np.random.permutation(data)\n",
    "    data_train, data_test = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(data_train)\n",
    "    X_test, y_test = split_X_y(data_test)\n",
    "\n",
    "    assert X_train.shape[0] == y_train.size\n",
    "    assert X_test.shape[0] == y_test.size\n",
    "\n",
    "    print(\"Train data set size = %d\" % X_train.shape[0])\n",
    "    print(\"Test data set size = %d\" % X_test.shape[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class KPOneVsOneClassifier:\n",
    "    def __init__(self, n_classes, d):\n",
    "        if n_classes <= 1:\n",
    "            raise Exception(\"Number of classes 'n_classes' must be greater or equal to 2\")\n",
    "        self.n_classes = n_classes\n",
    "        self.n_classifiers = n_classes * (n_classes - 1) // 2\n",
    "        self.d = d\n",
    "        self.X_train = None\n",
    "        self.Alpha = None\n",
    "        self.P = None\n",
    "        self.K = None\n",
    "\n",
    "    def _get_penalization_matrix(self, n_classes: int) -> np.ndarray:\n",
    "        n_classifiers = int(n_classes * (n_classes - 1) / 2)\n",
    "        P = np.zeros((n_classes, n_classifiers))\n",
    "        i_start, i_curr = 0, 1\n",
    "        for j in range(n_classifiers):\n",
    "            P[i_start, j] = 1\n",
    "            P[i_curr, j] = -1\n",
    "            if i_curr == n_classes - 1:\n",
    "                i_start += 1\n",
    "                i_curr = i_start + 1\n",
    "            else:\n",
    "                i_curr += 1\n",
    "        return P\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        return np.power(X_train @ X_train.T, self.d)\n",
    "\n",
    "    def sign(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x <= 0, -1, 1)\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs) -> None:\n",
    "        if self.P is None:\n",
    "            self.P = self._get_penalization_matrix(self.n_classes)\n",
    "\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "        self.Alpha = np.zeros((self.n_classifiers, X_train.shape[0]))\n",
    "        self.X_train = X_train\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each point, calculate confidence and make predictions\n",
    "            for t in range(0, self.X_train.shape[0]):\n",
    "                # Penalize classifiers that are responsible for predicting y_train[t]\n",
    "                y_actual = int(y_train[t])\n",
    "                y_preds = self.sign(self.Alpha @ self.K[t])\n",
    "                #                       0   if x1 < 0\n",
    "                # heaviside(x1, x2) =  x2   if x1 == 0\n",
    "                #                       1   if x1 > 0\n",
    "\n",
    "                # -1 on mistake, 1 correct, 0 on irrelevant perceptron\n",
    "                mistake_markers = (y_preds * self.P[y_actual, :])\n",
    "                self.Alpha[:, t] -= np.heaviside(-mistake_markers, 1) * y_preds\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        # Select majority vote, to break even use random selection\n",
    "        K_test = np.power(self.X_train @ X_test.T, self.d)\n",
    "        # Get votes form all classifiers into a vector\n",
    "        y_votes = np.sign(self.Alpha @ K_test)\n",
    "        # Sum votes by multiplying penalization matrix with votes vector\n",
    "        # this is the sum of votes and counter-votes (negative weight) for each digit\n",
    "        votes_sum = self.P @ y_votes\n",
    "        return np.argmax(votes_sum, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_error_percentage(y, y_preds):\n",
    "    error = 100 * get_num_mistakes(actual=y, predicted=y_preds) / y.size\n",
    "    # print(\"in-sample = % \" + str(error))\n",
    "    return error\n",
    "\n",
    "def get_num_mistakes(actual: np.ndarray, predicted: np.ndarray) -> int:\n",
    "    # or calculating by checking which alpha values are different than 0? alpha is 0 when the prediction matches\n",
    "    diffs = actual - predicted\n",
    "    n_mistakes = 0\n",
    "    for diff in diffs:\n",
    "        if diff != 0:\n",
    "            n_mistakes += 1\n",
    "    return n_mistakes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *_Part 1_*\n",
    "\n",
    "### 1. *Basic results for OVO*\n",
    "- for $d=1, ... ,7$ perform 20 runs\n",
    "- split $zipcombo$ 80-20\n",
    "- report MSE and STD\n",
    "- yield a 2x7 table that has on each cell $mean+-std$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We consider the learnings from the OVA algorithm, leaving n_epoch = 7\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 19\n",
      "took 1380.0990171432495\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "means_train = []\n",
    "means_test = []\n",
    "stds_train = []\n",
    "stds_test = []\n",
    "\n",
    "start=time.time()\n",
    "for d in range(1, 8):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for run in range(20):\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        kpova = KPOneVsOneClassifier(n_classes=10, d=d)\n",
    "\n",
    "        #train\n",
    "        kpova.fit(X_train, y_train, n_epochs=7)\n",
    "        y_insample = kpova.predict(X_train)\n",
    "        #test\n",
    "        y_outsample = kpova.predict(X_test)\n",
    "\n",
    "        print('\\nd=' + str(d) + ' on run ' + str(run))\n",
    "        train_errors.append(get_error_percentage(y_train, y_insample))\n",
    "        test_errors.append(get_error_percentage(y_test, y_outsample))\n",
    "    means_train.append(np.mean(train_errors))\n",
    "    means_test.append(np.mean(test_errors))\n",
    "    stds_train.append(np.std(train_errors))\n",
    "    stds_test.append(np.std(test_errors))\n",
    "end = time.time()\n",
    "\n",
    "print('took {t}'.format(t=end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "     mean train     mean test\n0  44.654±3.657  45.806±3.806\n1   6.808±1.532  10.935±1.458\n2   2.154±0.506   5.960±0.710\n3   1.071±0.194   4.858±0.479\n4   0.574±0.110   4.202±0.515\n5   0.536±0.093   4.073±0.418\n6   0.475±0.093   3.933±0.547",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean train</th>\n      <th>mean test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44.654±3.657</td>\n      <td>45.806±3.806</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.808±1.532</td>\n      <td>10.935±1.458</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.154±0.506</td>\n      <td>5.960±0.710</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.071±0.194</td>\n      <td>4.858±0.479</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.574±0.110</td>\n      <td>4.202±0.515</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.536±0.093</td>\n      <td>4.073±0.418</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.475±0.093</td>\n      <td>3.933±0.547</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_1a = pd.DataFrame({\n",
    "    'mean train': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_train, stds_train)],\n",
    "    'mean test': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_test, stds_test)],\n",
    "})\n",
    "\n",
    "display(table_1a)\n",
    "table_1a.to_csv('results_ovo/table_1a.csv')\n",
    "table_1a.style.to_latex('results_ovo/table_1a.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. _Cross validation for OVO_\n",
    "\n",
    "- we cross validate on range(4, 10) since we observed in the first experiment that the errors are still quite high until d=4\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def cross_validation(X, y, n_folds):\n",
    "    fold_size = X.shape[0] // n_folds\n",
    "\n",
    "    split_idxs = [i * fold_size - 1 for i in range(1, 5)]\n",
    "    X_folds_list = np.split(X, indices_or_sections=split_idxs)\n",
    "    y_folds_list = np.split(y, indices_or_sections=split_idxs)\n",
    "    assert len(X_folds_list) == len(y_folds_list) == n_folds\n",
    "\n",
    "    errors_d = []\n",
    "    ### BE CAREFUL AT THE OFFSET\n",
    "    for d in range(4, 11):\n",
    "        local_errors = []\n",
    "        for i in range(n_folds):\n",
    "            # Create a training and test folds from given data\n",
    "            X_train_fold = np.vstack(([X_folds_list[k] for k in range(0, n_folds) if k != i]))\n",
    "            y_train_fold = np.concatenate([y_folds_list[k] for k in range(0, n_folds) if k != i], axis=0)\n",
    "            X_validation_fold = X_folds_list[i]\n",
    "            y_validation_fold = y_folds_list[i]\n",
    "\n",
    "            kp = KPOneVsOneClassifier(n_classes=10, d=d)\n",
    "            kp.fit(X_train_fold, y_train_fold, n_epochs=7)\n",
    "            y_preds = kp.predict(X_validation_fold)\n",
    "\n",
    "            # find the hardest points to predict\n",
    "            train_diffs = y_validation_fold - y_preds\n",
    "            for pos in np.where(train_diffs!=0):\n",
    "                mistakes[pos] += 1\n",
    "\n",
    "            local_errors.append(get_error_percentage(y_validation_fold, y_preds))\n",
    "        errors_d.append(local_errors)\n",
    "\n",
    "    # +4 because we consider only ds starting from 4\n",
    "    return np.argmax(np.mean(errors_d, axis=1)) + 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1553, 1269,  929,  824,  852,  716,  834,  792,  708,  821])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of instances of each digit in the dataset\n",
    "# since we just reshuffle, the number holds for cross validation as well\n",
    "instances = np.array(np.zeros(10, dtype='int'))\n",
    "\n",
    "for y in y_train:\n",
    "    instances[int(y)]+=1\n",
    "for y in y_test:\n",
    "    instances[int(y)]+=1\n",
    "\n",
    "instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUN = 0\n"
     ]
    }
   ],
   "source": [
    "best_ds = []\n",
    "test_errors = []\n",
    "confusion_matrix = np.array(np.zeros((10, 10, 20)))\n",
    "mistakes = np.array(np.zeros(data.shape[0]))\n",
    "\n",
    "for run in range(20):\n",
    "    print('\\nRUN = '+str(run))\n",
    "    shuffled = np.random.permutation(data)\n",
    "    train_data, test_data = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(train_data)\n",
    "    X_test, y_test = split_X_y(test_data)\n",
    "\n",
    "    best_d = cross_validation(X_train, y_train, 5)\n",
    "\n",
    "    print('\\n      Best D = ' + str(best_d))\n",
    "    kpova = KPOneVsOneClassifier(n_classes=10, d=best_d)\n",
    "    kpova.fit(X_train, y_train, n_epochs=7)\n",
    "    y_outsample = kpova.predict(X_test)\n",
    "    test_error = get_error_percentage(y_test, y_outsample)\n",
    "\n",
    "    print('\\n      Retrain DONE')\n",
    "\n",
    "    # find the hardest points to predict\n",
    "    train_diffs = y_test - y_outsample\n",
    "    for pos in np.where(train_diffs!=0):\n",
    "        mistakes[pos] += 1\n",
    "\n",
    "    # get errors per digit for CONFUSION MATRIX\n",
    "    local_conf = np.array(np.zeros((10, 10)))\n",
    "    for (truth, pred) in zip(y_test, y_outsample):\n",
    "        if truth!=pred:\n",
    "            local_conf[int(truth), int(pred)] += 1\n",
    "\n",
    "    print('\\n      Confusion matrix DONE')\n",
    "    # in local_conf divide each row by instances[row]\n",
    "    # stack the entire matrix in the third dimension\n",
    "    confusion_matrix[:, :, run] = local_conf / instances[:, None]\n",
    "\n",
    "    best_ds.append(best_d)\n",
    "    test_errors.append(test_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(mistakes, 'results_ovo/mistakes_ovo.npy')\n",
    "\n",
    "# point 2 - CROSS VALIDATION\n",
    "ds_errors = pd.DataFrame({\n",
    "    'best_ds' : best_ds,\n",
    "    'test_errors' : test_errors,\n",
    "})\n",
    "\n",
    "print(str(f'{np.mean(best_ds):.3f}') + u\"\\u00B1\" + str(f'{np.std(best_ds):.3f}'))\n",
    "print(str(f'{np.mean(test_errors):.3f}') + u\"\\u00B1\" + str(f'{np.std(test_errors):.3f}'))\n",
    "\n",
    "ds_errors.to_csv('results_ovo/best_d_errors.csv')\n",
    "ds_errors.style.to_latex('results_ovo/best_d_errors.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# point 3 - CONFUSION MATRIX\n",
    "\n",
    "mean_confusion = confusion_matrix.mean(axis=2)\n",
    "std_confusion = confusion_matrix.std(axis=2)\n",
    "mean_std = np.array(np.zeros((10, 10))).astype('U')\n",
    "\n",
    "for i in range(mean_confusion.shape[0]):\n",
    "    mean_std[i] = [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(mean_confusion[i], std_confusion[i])]\n",
    "\n",
    "# save means and stddev separately\n",
    "pd.DataFrame(mean_confusion).to_csv('results_ovo/means_confusion.csv')\n",
    "pd.DataFrame(std_confusion).to_csv('results_ovo/stddev_confusion.csv')\n",
    "\n",
    "# save and print the combined matrices\n",
    "mean_std_df = pd.DataFrame(mean_std)\n",
    "mean_std_df.to_csv('results_ovo/confusion_matrix.csv')\n",
    "mean_std_df.style.to_latex('results_ovo/confusion_matrix.tex')\n",
    "\n",
    "mean_std_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# create confusion matrix heatmap to show more intuitive results\n",
    "sns.heatmap(mean_confusion, annot=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(std_confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. _TOP 5 hardest points to predict_\n",
    "\n",
    "- We use 20 runs of cross validation and average the results to yield the best $d$ in general, for any configuration of this dataset\n",
    "- Per run, cross_validation helps us find the best $d$ for that exact dataset configuration(shuffle).\n",
    "- Therefore, it is sound to amend the same algorithm to find the Top 5, even if we consider that $d$ might vary between the 20 runs.\n",
    "\n",
    "- PRO: what is the probability that in 20 runs, by counting the mistakes on 20% of the dataset, we actually take into account all points in a dataset?\n",
    "- CONTRA: you predict on the dataset it was also trained on, in the same order\n",
    "- The best way to determine this would be Leave One Out with the best d, training on all datasets points but 1(or n), and test on it. But the time did not permit for something like this\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_digit(grayscale):\n",
    "    plt.imshow(np.reshape(grayscale, (16, 16)), cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hard_points_labels = []\n",
    "hard_points = []\n",
    "\n",
    "for digit_index in np.argsort(mistakes)[-5:]:\n",
    "    label = data[digit_index][0]\n",
    "    point = data[digit_index][1:]\n",
    "    print(label)\n",
    "    display_digit(point)\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}