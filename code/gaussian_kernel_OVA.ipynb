{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = np.loadtxt('../support/zipcombo.dat')\n",
    "# data = np.loadtxt('../support/dtrain123.dat')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(9298, 257)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def split_80_20(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits 80% train 20% test\n",
    "\n",
    "    :param data: sequence.\n",
    "    :return: train_data, test_data: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]\n",
    "    train_size = int(n*0.8)\n",
    "    return data[:train_size], data[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def split_X_y(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits the data into datapoints and labels, X_train matrix and y_train;\n",
    "    :param data: np.ndarray\n",
    "    :return: X_train, y_train: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, 1:], data[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def shuffle_split(data):\n",
    "    # np.random.seed(seed)\n",
    "    shuffled = np.random.permutation(data)\n",
    "    data_train, data_test = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(data_train)\n",
    "    X_test, y_test = split_X_y(data_test)\n",
    "\n",
    "    assert X_train.shape[0] == y_train.size\n",
    "    assert X_test.shape[0] == y_test.size\n",
    "\n",
    "    print(\"Train data set size = %d\" % X_train.shape[0])\n",
    "    print(\"Test data set size = %d\" % X_test.shape[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_error_percentage(y, y_preds):\n",
    "    error = 100 * get_num_mistakes(actual=y, predicted=y_preds) / y.size\n",
    "    # print(\"in-sample = % \" + str(error))\n",
    "    return error\n",
    "\n",
    "def get_num_mistakes(actual: np.ndarray, predicted: np.ndarray) -> int:\n",
    "    # or calculating by checking which alpha values are different than 0? alpha is 0 when the prediction matches\n",
    "    diffs = actual - predicted\n",
    "    n_mistakes = 0\n",
    "    for diff in diffs:\n",
    "        if diff != 0:\n",
    "            n_mistakes += 1\n",
    "    return n_mistakes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took :5.632487058639526\n",
      "3.401452003226674\n",
      "7.688172043010753\n"
     ]
    }
   ],
   "source": [
    "class KPOneVsAllClassifier:\n",
    "    def __init__(self, n_classes, c):\n",
    "        self.n_classes = n_classes # could optimize to remove this var since =k.shape[0]\n",
    "        self.c = c\n",
    "        self.X_train = None\n",
    "        self.Alpha = None\n",
    "        self.K = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        ## we use ||x-y|| = np.inner(x-y, x-y)\n",
    "        ## and ||x-y||^2 = ||x||^2 + ||y||^2 - 2*x.T*y\n",
    "        X_norm = np.einsum('ij,ij->i', X_train, X_train)\n",
    "        return np.exp((-self.c) * (X_norm[:, None] + X_norm[None, :] - 2*np.dot(X_train, X_train.T)))\n",
    "\n",
    "    def _get_kernel_matrix_predict(self, X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n",
    "        ## we use ||x-y|| = np.inner(x-y, x-y)\n",
    "        ## and ||x-y||^2 = ||x||^2 + ||y||^2 - 2*x.T*y\n",
    "        X_test_norm = np.linalg.norm(X_test, axis=1, keepdims=False)\n",
    "        X_train_norm = np.linalg.norm(X_train, axis=1, keepdims=False)\n",
    "        return np.exp(-self.c * (X_test_norm[:, None] + X_train_norm[None, :] - 2 * X_test @ X_train.T))\n",
    "\n",
    "    def sign(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x <= 0, -1, 1)\n",
    "\n",
    "    def _predict_single_confidence(self, t, y_train):\n",
    "        # Get prediction array P\n",
    "        # preds = (self.Alpha @ self.K)[:, t]\n",
    "        preds = self.Alpha @ self.K[t]\n",
    "\n",
    "        # Get Y_t array of ground truth (duplicate y_t)\n",
    "        y = np.full(self.n_classes, -1)\n",
    "        y[int(y_train[t])] = 1\n",
    "\n",
    "        # penilize\n",
    "        self.Alpha[:, t] -= np.heaviside(-(preds * y), 1) * self.sign(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs):\n",
    "        y_preds = np.array(np.zeros(X_train.shape[0]))\n",
    "\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "        self.Alpha = np.zeros((self.n_classes, X_train.shape[0]))\n",
    "        self.X_train = X_train\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each point, calculate confidence and make predictions\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                confidence = self._predict_single_confidence(t, y_train)\n",
    "                # the index with the highest number(confidence) is the prediction\n",
    "                # +1 because the index for confidence in perceptron[1] is 0\n",
    "                y_preds[t] = np.argmax(confidence)\n",
    "        return y_preds\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        self.K_test = self._get_kernel_matrix_predict(self.X_train, X_test)\n",
    "        return np.argmax((self.Alpha @ self.K_test.T), axis=0)\n",
    "\n",
    "## for in-cell debug\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(n_classes=10, c=0.02)\n",
    "kpova.fit(X_train, y_train, n_epochs=7)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end-start))\n",
    "\n",
    "print(get_error_percentage(y_train, y_insample))\n",
    "print(get_error_percentage(y_test, y_outsample))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Repeat 1 and 2\n",
    "- do the parameter tuning c\n",
    "- first, do some experiments to decide the values to cross_validate over for c\n",
    "- since gamma, i.e. $c$ is not as sensible as $d$ is in polynomial kernel, we make a with multiples of 10. Practice observed use the same approach\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First, we run the 1.basic results algorithm to get an intuition of how $c$ impacts the errors, so that we can find out on which $S$ to cross validate. We set n_epochs=7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KPOneVsAllClassifier.__init__() got an unexpected keyword argument 'd'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m run \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m20\u001B[39m):\n\u001B[0;32m     15\u001B[0m     X_train, y_train, X_test, y_test \u001B[38;5;241m=\u001B[39m shuffle_split(data)\n\u001B[1;32m---> 17\u001B[0m     kpova \u001B[38;5;241m=\u001B[39m \u001B[43mKPOneVsAllClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m#train\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     kpova\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: KPOneVsAllClassifier.__init__() got an unexpected keyword argument 'd'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "means_train = []\n",
    "means_test = []\n",
    "stds_train = []\n",
    "stds_test = []\n",
    "\n",
    "c_vals = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "start=time.time()\n",
    "for c in c_vals:\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for run in range(20):\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        kpova = KPOneVsAllClassifier(n_classes=10, c=c)\n",
    "\n",
    "        #train\n",
    "        kpova.fit(X_train, y_train, n_epochs=7)\n",
    "        y_insample = kpova.predict(X_train)\n",
    "        #test\n",
    "        y_outsample = kpova.predict(X_test)\n",
    "\n",
    "        print('\\nd=' + str(c) + ' on run ' + str(run))\n",
    "        train_errors.append(get_error_percentage(y_train, y_insample))\n",
    "        test_errors.append(get_error_percentage(y_test, y_outsample))\n",
    "    means_train.append(np.mean(train_errors))\n",
    "    means_test.append(np.mean(test_errors))\n",
    "    stds_train.append(np.std(train_errors))\n",
    "    stds_test.append(np.std(test_errors))\n",
    "end = time.time()\n",
    "\n",
    "print('took {t}'.format(t=end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "     mean train     mean test\n0  84.369±3.756  84.016±4.142\n1  74.118±4.311  74.430±4.469\n2  50.532±6.883  51.527±7.229\n3  30.083±4.614  31.387±4.848\n4  19.428±3.843  20.987±3.950\n5  12.918±3.275  14.602±3.402\n6   8.842±1.385  10.855±1.587",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean train</th>\n      <th>mean test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84.369±3.756</td>\n      <td>84.016±4.142</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>74.118±4.311</td>\n      <td>74.430±4.469</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50.532±6.883</td>\n      <td>51.527±7.229</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30.083±4.614</td>\n      <td>31.387±4.848</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19.428±3.843</td>\n      <td>20.987±3.950</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12.918±3.275</td>\n      <td>14.602±3.402</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8.842±1.385</td>\n      <td>10.855±1.587</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_1a = pd.DataFrame({\n",
    "    'c': c_vals,\n",
    "    'mean train': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_train, stds_train)],\n",
    "    'mean test': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_test, stds_test)],\n",
    "})\n",
    "\n",
    "display(table_1a)\n",
    "table_1a.to_csv('results_gaussian/table_1a.csv')\n",
    "table_1a.style.to_latex('results_gaussian/table_1a.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we have to decide S, the range to cross validate over parameter c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def cross_validation(X, y, n_folds):\n",
    "    fold_size = X.shape[0] // n_folds\n",
    "\n",
    "    split_idxs = [i * fold_size - 1 for i in range(1, 5)]\n",
    "    X_folds_list = np.split(X, indices_or_sections=split_idxs)\n",
    "    y_folds_list = np.split(y, indices_or_sections=split_idxs)\n",
    "    assert len(X_folds_list) == len(y_folds_list) == n_folds\n",
    "\n",
    "    errors_d = []\n",
    "    ### BE CAREFUL AT THE OFFSET\n",
    "    for d in range(3, 8):\n",
    "        local_errors = []\n",
    "        for i in range(n_folds):\n",
    "            # Create a training and test folds from given data\n",
    "            X_train_fold = np.vstack(([X_folds_list[k] for k in range(0, n_folds) if k != i]))\n",
    "            y_train_fold = np.concatenate([y_folds_list[k] for k in range(0, n_folds) if k != i], axis=0)\n",
    "            X_validation_fold = X_folds_list[i]\n",
    "            y_validation_fold = y_folds_list[i]\n",
    "\n",
    "            kp = KPOneVsAllClassifier(n_classes=10, d=d)\n",
    "            kp.fit(X_train_fold, y_train_fold, n_epochs=7)\n",
    "            y_preds = kp.predict(X_validation_fold)\n",
    "\n",
    "            # find the hardest points to predict\n",
    "            train_diffs = y_validation_fold - y_preds\n",
    "            for pos in np.where(train_diffs!=0):\n",
    "                mistakes[pos] += 1\n",
    "\n",
    "            local_errors.append(get_error_percentage(y_validation_fold, y_preds))\n",
    "        errors_d.append(local_errors)\n",
    "\n",
    "    # +3 because we consider only ds starting from 3\n",
    "    return np.argmax(np.mean(errors_d, axis=1)) + 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1553, 1269,  929,  824,  852,  716,  834,  792,  708,  821])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of instances of each digit in the dataset\n",
    "# since we just reshuffle, the number holds for cross validation as well\n",
    "instances = np.array(np.zeros(10, dtype='int'))\n",
    "\n",
    "for y in y_train:\n",
    "    instances[int(y)]+=1\n",
    "for y in y_test:\n",
    "    instances[int(y)]+=1\n",
    "\n",
    "instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUN = 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m X_train, y_train \u001B[38;5;241m=\u001B[39m split_X_y(train_data)\n\u001B[0;32m     11\u001B[0m X_test, y_test \u001B[38;5;241m=\u001B[39m split_X_y(test_data)\n\u001B[1;32m---> 13\u001B[0m best_d \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m      Best D = \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(best_d))\n\u001B[0;32m     16\u001B[0m kpova \u001B[38;5;241m=\u001B[39m KPOneVsAllClassifier(n_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, d\u001B[38;5;241m=\u001B[39mbest_d)\n",
      "Cell \u001B[1;32mIn[19], line 21\u001B[0m, in \u001B[0;36mcross_validation\u001B[1;34m(X, y, n_folds)\u001B[0m\n\u001B[0;32m     18\u001B[0m y_validation_fold \u001B[38;5;241m=\u001B[39m y_folds_list[i]\n\u001B[0;32m     20\u001B[0m kp \u001B[38;5;241m=\u001B[39m KPOneVsAllClassifier(n_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, d\u001B[38;5;241m=\u001B[39md)\n\u001B[1;32m---> 21\u001B[0m \u001B[43mkp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_fold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_fold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m y_preds \u001B[38;5;241m=\u001B[39m kp\u001B[38;5;241m.\u001B[39mpredict(X_validation_fold)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# find the hardest points to predict\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 45\u001B[0m, in \u001B[0;36mKPOneVsAllClassifier.fit\u001B[1;34m(self, X_train, y_train, n_epochs)\u001B[0m\n\u001B[0;32m     42\u001B[0m         confidence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predict_single_confidence(t, y_train)\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;66;03m# the index with the highest number(confidence) is the prediction\u001B[39;00m\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;66;03m# +1 because the index for confidence in perceptron[1] is 0\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m         y_preds[t] \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfidence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y_preds\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\OneDrive - University College London\\Documents_pc\\University\\4th_yr\\1st_term\\COMP0078\\cw2\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(a, axis, out, keepdims)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1130\u001B[0m \u001B[38;5;124;03mReturns the indices of the maximum values along an axis.\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;124;03m(2, 1, 4)\u001B[39;00m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m kwds \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeepdims\u001B[39m\u001B[38;5;124m'\u001B[39m: keepdims} \u001B[38;5;28;01mif\u001B[39;00m keepdims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39m_NoValue \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margmax\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\OneDrive - University College London\\Documents_pc\\University\\4th_yr\\1st_term\\COMP0078\\cw2\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_ds = []\n",
    "test_errors = []\n",
    "confusion_matrix = np.array(np.zeros((10, 10, 20)))\n",
    "mistakes = np.array(np.zeros(data.shape[0]))\n",
    "\n",
    "for run in range(20):\n",
    "    print('\\nRUN = '+str(run))\n",
    "    shuffled = np.random.permutation(data)\n",
    "    train_data, test_data = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(train_data)\n",
    "    X_test, y_test = split_X_y(test_data)\n",
    "\n",
    "    best_d = cross_validation(X_train, y_train, 5)\n",
    "\n",
    "    print('\\n      Best D = ' + str(best_d))\n",
    "    kpova = KPOneVsAllClassifier(n_classes=10, d=best_d)\n",
    "    kpova.fit(X_train, y_train, n_epochs=7)\n",
    "    y_outsample = kpova.predict(X_test)\n",
    "    test_error = get_error_percentage(y_test, y_outsample)\n",
    "\n",
    "    print('\\n      Retrain DONE')\n",
    "\n",
    "    # get errors per digit for CONFUSION MATRIX\n",
    "    local_conf = np.array(np.zeros((10, 10)))\n",
    "    for (truth, pred) in zip(y_test, y_outsample):\n",
    "        if truth!=pred:\n",
    "            local_conf[int(truth), int(pred)] += 1\n",
    "\n",
    "    print('\\n      Confusion matrix DONE')\n",
    "    # in local_conf divide each row by instances[row]\n",
    "    # stack the entire matrix in the third dimension\n",
    "    confusion_matrix[:, :, run] = local_conf / instances[:, None]\n",
    "\n",
    "    best_ds.append(best_d)\n",
    "    test_errors.append(test_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.savetxt('results_gaussian/mistakes_ova.npy', mistakes)\n",
    "\n",
    "# point 2 - CROSS VALIDATION\n",
    "ds_errors = pd.DataFrame({\n",
    "    'best_ds' : best_ds,\n",
    "    'test_errors' : test_errors,\n",
    "})\n",
    "\n",
    "print(str(f'{np.mean(best_ds):.3f}') + u\"\\u00B1\" + str(f'{np.std(best_ds):.3f}'))\n",
    "print(str(f'{np.mean(test_errors):.3f}') + u\"\\u00B1\" + str(f'{np.std(test_errors):.3f}'))\n",
    "\n",
    "ds_errors.to_csv('results_gaussian/best_d_errors.csv')\n",
    "ds_errors.style.to_latex('results_gaussian/best_d_errors.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# point 3 - CONFUSION MATRIX\n",
    "\n",
    "mean_confusion = confusion_matrix.mean(axis=2)*100\n",
    "std_confusion = confusion_matrix.std(axis=2)*100\n",
    "mean_std = np.array(np.zeros((10, 10))).astype('U')\n",
    "\n",
    "for i in range(mean_confusion.shape[0]):\n",
    "    mean_std[i] = [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(mean_confusion[i], std_confusion[i])]\n",
    "\n",
    "# save means and stddev separately\n",
    "pd.DataFrame(mean_confusion).to_csv('results_gaussian/means_confusion.csv')\n",
    "pd.DataFrame(std_confusion).to_csv('results_gaussian/stddev_confusion.csv')\n",
    "\n",
    "# save and print the combined matrices\n",
    "mean_std_df = pd.DataFrame(mean_std)\n",
    "mean_std_df.to_csv('results_gaussian/confusion_matrix.csv')\n",
    "mean_std_df.style.to_latex('results_gaussian/confusion_matrix.tex')\n",
    "\n",
    "mean_std_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# create confusion matrix heatmap to show more intuitive results\n",
    "sns.heatmap(mean_confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(std_confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_digit(grayscale):\n",
    "    plt.imshow(np.reshape(grayscale, (16, 16)), cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hard_points_labels = []\n",
    "hard_points = []\n",
    "\n",
    "for digit_index in np.argsort(mistakes)[-5:]:\n",
    "    label = data[digit_index][0]\n",
    "    point = data[digit_index][1:]\n",
    "    print(label)\n",
    "    display_digit(point)\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}