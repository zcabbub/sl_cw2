{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = np.loadtxt('../support/zipcombo.dat')\n",
    "# data = np.loadtxt('../support/dtrain123.dat')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(9298, 257)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def split_80_20(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits 80% train 20% test\n",
    "\n",
    "    :param data: sequence.\n",
    "    :return: train_data, test_data: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]\n",
    "    train_size = int(n*0.8)\n",
    "    return data[:train_size], data[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def split_X_y(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits the data into datapoints and labels, X_train matrix and y_train;\n",
    "    :param data: np.ndarray\n",
    "    :return: X_train, y_train: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, 1:], data[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def shuffle_split(data):\n",
    "    # np.random.seed(seed)\n",
    "    shuffled = np.random.permutation(data)\n",
    "    data_train, data_test = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(data_train)\n",
    "    X_test, y_test = split_X_y(data_test)\n",
    "\n",
    "    assert X_train.shape[0] == y_train.size\n",
    "    assert X_test.shape[0] == y_test.size\n",
    "\n",
    "    print(\"Train data set size = %d\" % X_train.shape[0])\n",
    "    print(\"Test data set size = %d\" % X_test.shape[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_error_percentage(y, y_preds):\n",
    "    error = 100 * get_num_mistakes(actual=y, predicted=y_preds) / y.size\n",
    "    # print(\"in-sample = % \" + str(error))\n",
    "    return error\n",
    "\n",
    "def get_num_mistakes(actual: np.ndarray, predicted: np.ndarray) -> int:\n",
    "    # or calculating by checking which alpha values are different than 0? alpha is 0 when the prediction matches\n",
    "    diffs = actual - predicted\n",
    "    n_mistakes = 0\n",
    "    for diff in diffs:\n",
    "        if diff != 0:\n",
    "            n_mistakes += 1\n",
    "    return n_mistakes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took :5.453933238983154\n",
      "7.098682441516536\n",
      "8.279569892473118\n"
     ]
    }
   ],
   "source": [
    "class KPOneVsAllClassifier:\n",
    "    def __init__(self, n_classes, d):\n",
    "        self.n_classes = n_classes # could optimize to remove this var since =k.shape[0]\n",
    "        self.d = d\n",
    "        self.X_train = None\n",
    "        self.Alpha = None\n",
    "        self.K = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        ## we use ||x-y|| = np.inner(x-y, x-y)\n",
    "        ## and ||x-y||^2 = ||x||^2 + ||y||^2 - 2*x.T*y\n",
    "        X_norm = np.einsum('ij,ij->i', X_train, X_train)\n",
    "        return np.exp((-self.d) * (X_norm[:, None] + X_norm[None, :] - 2*np.dot(X_train, X_train.T)))\n",
    "\n",
    "    def sign(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x <= 0, -1, 1)\n",
    "\n",
    "    def _predict_single_confidence(self, t, y_train):\n",
    "        # Get prediction array P\n",
    "        # preds = (self.Alpha @ self.K)[:, t]\n",
    "        preds = self.Alpha @ self.K[t]\n",
    "\n",
    "        # Get Y_t array of ground truth (duplicate y_t)\n",
    "        y = np.full(self.n_classes, -1)\n",
    "        y[int(y_train[t])] = 1\n",
    "\n",
    "        # penilize\n",
    "        self.Alpha[:, t] -= np.heaviside(-(preds * y), 1) * self.sign(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs):\n",
    "        y_preds = np.array(np.zeros(X_train.shape[0]))\n",
    "\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "        self.Alpha = np.zeros((self.n_classes, X_train.shape[0]))\n",
    "        self.X_train = X_train\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each point, calculate confidence and make predictions\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                confidence = self._predict_single_confidence(t, y_train)\n",
    "                # the index with the highest number(confidence) is the prediction\n",
    "                # +1 because the index for confidence in perceptron[1] is 0\n",
    "                y_preds[t] = np.argmax(confidence)\n",
    "        return y_preds\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        ## for polynomial kernel\n",
    "        self.K_test = np.power(self.X_train @ X_test.T, self.d)\n",
    "        return np.argmax((self.Alpha @ self.K_test), axis=0)\n",
    "\n",
    "\n",
    "## for in-cell debug\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(n_classes=10, d=10)\n",
    "kpova.fit(X_train, y_train, n_epochs=1)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end-start))\n",
    "\n",
    "print(get_error_percentage(y_train, y_insample))\n",
    "print(get_error_percentage(y_test, y_outsample))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Repeat 1 and 2\n",
    "- do the parameter tuning d = c\n",
    "- first, do some experiments to decide the values to cross_validate over for c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First, we run the 1.basic results algorithm to get an intuition of how $c$ impacts the errors, so that we can find out on which $S$ to cross validate. We set n_epochs=7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 19\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 1\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 2\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 3\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 4\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 5\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 6\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 7\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 8\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 9\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 10\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 11\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 12\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 13\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 14\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 15\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 16\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 17\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 18\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 19\n",
      "took 1063.0187788009644\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "means_train = []\n",
    "means_test = []\n",
    "stds_train = []\n",
    "stds_test = []\n",
    "\n",
    "start=time.time()\n",
    "for c in range(1, 8):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for run in range(20):\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        kpova = KPOneVsAllClassifier(n_classes=10, d=c)\n",
    "\n",
    "        #train\n",
    "        kpova.fit(X_train, y_train, n_epochs=7)\n",
    "        y_insample = kpova.predict(X_train)\n",
    "        #test\n",
    "        y_outsample = kpova.predict(X_test)\n",
    "\n",
    "        print('\\nd=' + str(c) + ' on run ' + str(run))\n",
    "        train_errors.append(get_error_percentage(y_train, y_insample))\n",
    "        test_errors.append(get_error_percentage(y_test, y_outsample))\n",
    "    means_train.append(np.mean(train_errors))\n",
    "    means_test.append(np.mean(test_errors))\n",
    "    stds_train.append(np.std(train_errors))\n",
    "    stds_test.append(np.std(test_errors))\n",
    "end = time.time()\n",
    "\n",
    "print('took {t}'.format(t=end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean train    mean test\n0  7.267±1.561  9.374±1.434\n1  0.436±0.234  3.696±0.399\n2  0.111±0.054  2.798±0.405\n3  0.028±0.012  2.788±0.386\n4  0.233±0.639  3.008±0.621\n5  0.090±0.297  2.685±0.480\n6  0.024±0.011  2.656±0.352",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean train</th>\n      <th>mean test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.267±1.561</td>\n      <td>9.374±1.434</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.436±0.234</td>\n      <td>3.696±0.399</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111±0.054</td>\n      <td>2.798±0.405</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.028±0.012</td>\n      <td>2.788±0.386</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.233±0.639</td>\n      <td>3.008±0.621</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.090±0.297</td>\n      <td>2.685±0.480</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.024±0.011</td>\n      <td>2.656±0.352</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_1a = pd.DataFrame({\n",
    "    'mean train': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_train, stds_train)],\n",
    "    'mean test': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_test, stds_test)],\n",
    "})\n",
    "\n",
    "display(table_1a)\n",
    "table_1a.to_csv('table_1a.csv')\n",
    "table_1a.style.to_latex('table_1a.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def cross_validation(X, y, n_folds):\n",
    "    fold_size = X.shape[0] // n_folds\n",
    "\n",
    "    split_idxs = [i * fold_size - 1 for i in range(1, 5)]\n",
    "    X_folds_list = np.split(X, indices_or_sections=split_idxs)\n",
    "    y_folds_list = np.split(y, indices_or_sections=split_idxs)\n",
    "    assert len(X_folds_list) == len(y_folds_list) == n_folds\n",
    "\n",
    "    errors_d = []\n",
    "    for d in range(3, 8):\n",
    "        local_errors = []\n",
    "        for i in range(n_folds):\n",
    "            # Create a training and test folds from given data\n",
    "            X_train_fold = np.vstack(([X_folds_list[k] for k in range(0, n_folds) if k != i]))\n",
    "            y_train_fold = np.concatenate([y_folds_list[k] for k in range(0, n_folds) if k != i], axis=0)\n",
    "            X_validation_fold = X_folds_list[i]\n",
    "            y_validation_fold = y_folds_list[i]\n",
    "\n",
    "            kp = KPOneVsAllClassifier(n_classes=10, d=d)\n",
    "            kp.fit(X_train_fold, y_train_fold, n_epochs=7)\n",
    "            y_preds = kp.predict(X_validation_fold)\n",
    "\n",
    "            # find the hardest points to predict\n",
    "            train_diffs = y_validation_fold - y_preds\n",
    "            for pos in np.where(train_diffs!=0):\n",
    "                mistakes[pos] += 1\n",
    "\n",
    "            local_errors.append(get_error_percentage(y_validation_fold, y_preds))\n",
    "        errors_d.append(local_errors)\n",
    "\n",
    "    # +4 because we consider only ds starting from 4\n",
    "    return np.argmax(np.mean(errors_d, axis=1)) + 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}