{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing all modules in cell below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = np.loadtxt('../support/zipcombo.dat')\n",
    "# data = np.loadtxt('../support/dtrain123.dat')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(9298, 257)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define functions for splitting the dataset into train/test and input/label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def split_80_20(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits 80% train 20% test\n",
    "\n",
    "    :param data: sequence.\n",
    "    :return: train_data, test_data: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]\n",
    "    train_size = int(n*0.8)\n",
    "    return data[:train_size], data[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def split_X_y(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits the data into datapoints and labels, X_train matrix and y_train;\n",
    "    :param data: np.ndarray\n",
    "    :return: X_train, y_train: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, 1:], data[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def shuffle_split(data):\n",
    "    # np.random.seed(seed)\n",
    "    shuffled = np.random.permutation(data)\n",
    "    data_train, data_test = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(data_train)\n",
    "    X_test, y_test = split_X_y(data_test)\n",
    "\n",
    "    assert X_train.shape[0] == y_train.size\n",
    "    assert X_test.shape[0] == y_test.size\n",
    "\n",
    "    print(\"Train data set size = %d\" % X_train.shape[0])\n",
    "    print(\"Test data set size = %d\" % X_test.shape[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def display_digit(grayscale):\n",
    "    plt.imshow(np.reshape(grayscale, (16, 16)), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(X_train[100])\n",
    "print(y_train[100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now define the Kernel Perceptron algorithm and kernel functions to be used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def polynomial_kernel(x_i: np.ndarray, x_t: np.ndarray, d: int):\n",
    "    return np.inner(x_i, x_t) ** d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def gaussian_kernel(x_i: np.ndarray, x_j: np.ndarray, sigma=1):\n",
    "    if x_i.shape[0] != x_j.shape[0]:\n",
    "        raise Exception(\"Cannot apply kernel to vectors of different dimensions: x_i has shape {s1}, x_j has shape {s2}\"\n",
    "                        .format(s1=x_i.shape, s2=x_j.shape))\n",
    "    diff = x_i - x_j\n",
    "    return np.exp(-1 * np.inner(diff, diff) / (2 * sigma**2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, kernel: Callable[[np.ndarray, np.ndarray], float], size: int):\n",
    "        self.kernel = kernel\n",
    "        self.K = None\n",
    "        self.alpha = np.zeros(size)\n",
    "        self.X_train = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "\n",
    "        for i in range(0, X_train.shape[0]):\n",
    "            for j in range(i, X_train.shape[0]):\n",
    "                K[i, j] = K[j, i] = self.kernel(X_train[i], X_train[j])\n",
    "\n",
    "        return K\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray, n_epochs: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prediction function that populates the alpha parameter, 1 data point at a time;\n",
    "        :param X_train: training data points\n",
    "        :param y_train: training corresponding labels\n",
    "        :param n_epochs: number of times to pass through the data. Alpha contains the learning parameters that get inherited from one epoch to the other\n",
    "        :return: y_preds: the predictions enhanced after N epochs\n",
    "\n",
    "        \"\"\"\n",
    "        if y_train.ndim != 1:\n",
    "            raise Exception('y_train must be a 1-dim np.ndarray. Given y_train with shape {s}'.format(s=y_train.shape))\n",
    "        if X_train.shape[0] != y_train.size:\n",
    "            raise Exception('X_train and y_train must contain equal number of samples. Given X_train with shape {s1} and y_train with shape {s2}'.format(s1=y_train.shape, s2=X_train.shape))\n",
    "\n",
    "        y_preds = np.zeros(X_train.shape[0])\n",
    "        self.X_train = X_train\n",
    "\n",
    "        # Compute kernel matrix which stays constant throughout the algorithm and all epochs\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "\n",
    "        for epoch in range(0, n_epochs):\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                y_pred = self._predict_single(t)\n",
    "                y_preds[t] = y_pred\n",
    "                if y_pred != y_train[t]:\n",
    "                    self.alpha[t] += y_train[t]\n",
    "        return y_preds\n",
    "\n",
    "    def _predict_single(self, t: int):\n",
    "        \"\"\"\n",
    "        We take a whole row of kernel matrix K because in we want to account for errors in previous epochs.\n",
    "        This is not an issue in the first epoch because all alpha's > t are 0.\n",
    "        :param t: iteration step in the online learning algorithm.\n",
    "        :return: y hat, single predicted value at step t.\n",
    "        \"\"\"\n",
    "        return np.sign(np.inner(self.K[t, :], self.alpha))\n",
    "\n",
    "    def yhat_single(self, K, t: int):\n",
    "        return np.sign(np.inner(K[t, :], self.alpha))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prediction function to be used for out-of-sample test data point.\n",
    "        Does not perform online learning (update step).\n",
    "        :param X: test data points.\n",
    "        :return: np.ndarray of predictions for each given test data point.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x_test in X:\n",
    "            sum = 0\n",
    "            for t in range(self.X_train.shape[0]):\n",
    "                # Can improve by using kernel matrix\n",
    "                sum += self.alpha[t] * self.kernel(self.X_train[t], x_test)\n",
    "            predictions.append(np.sign(sum))\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# kp = KernelPerceptron(kernel=partial(polynomial_kernel, d=3))\n",
    "# y_insample = kp.fit(X_train_12, y_train_12, n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took :5.170046091079712\n",
      "in-sample = % 1.6402258671685936\n",
      "in-sample = % 3.870967741935484\n"
     ]
    },
    {
     "data": {
      "text/plain": "3.870967741935484"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_error_percentage_old(y_train, y_insample, y_test, y_outsample):\n",
    "    training_error = 100 * get_num_mistakes(actual=y_train, predicted=y_insample) / y_train.size\n",
    "    test_errors = 100 * get_num_mistakes(actual=y_test, predicted=y_outsample) / y_test.size\n",
    "    print(\"in-sample = % \" + str(training_error))\n",
    "    print(\"out-of-sample = % \" + str(test_errors))\n",
    "    return training_error, test_errors\n",
    "\n",
    "def get_error_percentage(y, y_preds):\n",
    "    error = 100 * get_num_mistakes(actual=y, predicted=y_preds) / y.size\n",
    "    print(\"in-sample = % \" + str(error))\n",
    "    return error\n",
    "\n",
    "def get_num_mistakes(actual: np.ndarray, predicted: np.ndarray) -> int:\n",
    "    # or calculating by checking which alpha values are different than 0? alpha is 0 when the prediction matches\n",
    "    diffs = actual - predicted\n",
    "    n_mistakes = 0\n",
    "    for diff in diffs:\n",
    "        if diff != 0:\n",
    "            n_mistakes += 1\n",
    "    return n_mistakes\n",
    "\n",
    "class KPOneVsAllClassifier:\n",
    "    def __init__(self, kernel, n_classes, d):\n",
    "        self.kernel = kernel\n",
    "        self.n_classes = n_classes # could optimize to remove this var since =k.shape[0]\n",
    "        self.d = d\n",
    "        self.X_train = None\n",
    "        self.Alpha = None\n",
    "        self.K = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        ### TODO: Takes a lot of time, we should improve efficiency\n",
    "        # K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "        # for i in range(0, X_train.shape[0]):\n",
    "        #     for j in range(i, X_train.shape[0]):\n",
    "        #         K[i, j] = K[j, i] = self.kernel(X_train[i], X_train[j], d=self.d)\n",
    "        # return K\n",
    "        return np.power(X_train @ X_train.T, self.d)\n",
    "\n",
    "    def sign(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x <= 0, -1, 1)\n",
    "\n",
    "    def _predict_single_confidence(self, t, y_train):\n",
    "        # Get prediction array P\n",
    "        # preds = (self.Alpha @ self.K)[:, t]\n",
    "        preds = self.Alpha @ self.K[t]\n",
    "\n",
    "        # Get Y_t array of ground truth (duplicate y_t)\n",
    "        y = np.full(self.n_classes, -1)\n",
    "        y[int(y_train[t])] = 1\n",
    "\n",
    "        # penilize\n",
    "        self.Alpha[:, t] -= np.heaviside(-(preds * y), 1) * self.sign(preds)\n",
    "        return preds\n",
    "        # for each perceptron\n",
    "        # for cl in range(self.n_classes):\n",
    "        #     # create and save predictions\n",
    "        #     pred = np.inner(self.K[t, :], perceptrons[cl].alpha)\n",
    "        #     confidence[cl] = pred\n",
    "        #\n",
    "        #     y = 1 if y_train[t] == float(cl) else (-1)\n",
    "        #     ## penalize if the prediction wrong\n",
    "        #     if y*pred <= 0:\n",
    "        #         perceptrons[cl].alpha[t] -= self.sign(pred)\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs):\n",
    "        y_preds = np.array(np.zeros(X_train.shape[0]))\n",
    "\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "        self.Alpha = np.zeros((self.n_classes, X_train.shape[0]))\n",
    "        self.X_train = X_train\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each point, calculate confidence and make predictions\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                confidence = self._predict_single_confidence(t, y_train)\n",
    "                # the index with the highest number(confidence) is the prediction\n",
    "                # +1 because the index for confidence in perceptron[1] is 0\n",
    "                y_preds[t] = np.argmax(confidence)\n",
    "        return y_preds\n",
    "\n",
    "    # def _choose_best_ytest(self, yhats):\n",
    "    #     # get the classifiers that predicted positive\n",
    "    #     maxims = np.argwhere(yhats == np.amax(yhats)).flatten() + 1\n",
    "    #     if maxims.shape[0] == 1:\n",
    "    #         # if only 1 classifier predicts positive, take that class\n",
    "    #          return maxims[0]\n",
    "    #     else:\n",
    "    #         # if more than one predicted positive, or all of them negative, choose one randomly\n",
    "    #         return np.random.choice(maxims)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        ## for polynomial kernel\n",
    "        self.K_test = np.power(X_train @ X_test.T, self.d)\n",
    "        return np.argmax((self.Alpha @ self.K_test), axis=0)\n",
    "        #\n",
    "        # for t_test in range(X_test.shape[0]):\n",
    "        #     yhats_sums = self.Alpha @ self.K_test[t_test]\n",
    "        #     ypreds[t_test] = self._choose_best_ytest(yhats_sums)\n",
    "        #     yhats_sums = np.array(np.zeros(self.n_classes))\n",
    "        #     for cl in range(self.n_classes):\n",
    "        #         # update\n",
    "        #         sum = np.inner(self.Alpha[:, t])\n",
    "        #         for t in range(self.X_train.shape[0]):\n",
    "        #             sum += self.perceptrons[cl].alpha[t] * self.kernel(self.X_train[t], X_test[t_test], self.d)\n",
    "        #         yhats_sums[cl] = sum\n",
    "        # return ypreds\n",
    "\n",
    "\n",
    "### for in-cell debug\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=3)\n",
    "kpova.fit(X_train, y_train, n_epochs=1)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end-start))\n",
    "\n",
    "get_error_percentage(y_train, y_insample)\n",
    "get_error_percentage(y_test, y_outsample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Tests_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us test the Kernel Perceptron implementation on a dummy dataset of only digits 1 and 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Filter out digit 3 (leave only digit 1 and 2) from dtrain123.dat train and test dataset\n",
    "indxs_digit_3 = np.where(y_train_123 == 3)\n",
    "X_train_12 = np.delete(X_train_123, indxs_digit_3, axis=0)\n",
    "y_train_12 = np.delete(y_train_123, indxs_digit_3)\n",
    "y_train_12[y_train_12 == 1] = -1\n",
    "y_train_12[y_train_12 == 2] = 1\n",
    "\n",
    "indxs_digit_3 = np.where(y_test_123 == 3)\n",
    "X_test_12 = np.delete(X_test_123, indxs_digit_3, axis=0)\n",
    "y_test_12 = np.delete(y_test_123, indxs_digit_3)\n",
    "y_test_12[y_test_12 == 1] = -1\n",
    "y_test_12[y_test_12 == 2] = 1\n",
    "\n",
    "assert X_train_12.shape[0] == y_train_12.size\n",
    "assert X_test_12.shape[0] == y_test_12.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample = % 0.0\n",
      "out-of-sample = % 1.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "# Test fitting and in-sample predictions\n",
    "kp = KernelPerceptron(kernel=partial(polynomial_kernel, d=5), size=X_train_12.shape[0])\n",
    "y_insample = kp.fit(X_train_12, y_train_12, n_epochs=2)\n",
    "# Test out-of-sample predictions\n",
    "y_outsample = kp.predict(X_test_12) # all predictions of test sample in y_outsample\n",
    "print(\"in-sample = % \" + str(100 * get_num_mistakes(actual=y_train_12, predicted=y_insample) / y_train_12.size))\n",
    "print(\"out-of-sample = % \" + str(100 * get_num_mistakes(actual=y_test_12, predicted=y_outsample) / y_test_12.size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KernelPerceptron.__init__() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m## Test the kernel matrix\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m kp_test \u001B[38;5;241m=\u001B[39m \u001B[43mKernelPerceptron\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolynomial_kernel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m kernel_matrix \u001B[38;5;241m=\u001B[39m kp_test\u001B[38;5;241m.\u001B[39m_get_kernel_matrix(X_train_12)\n\u001B[0;32m      4\u001B[0m polynomial_kernel(X_train_12[\u001B[38;5;241m0\u001B[39m], X_train_12[\u001B[38;5;241m0\u001B[39m], d\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m) \u001B[38;5;241m==\u001B[39m kernel_matrix[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: KernelPerceptron.__init__() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": [
    "## Test the kernel matrix\n",
    "kp_test = KernelPerceptron(kernel=partial(polynomial_kernel, d=3))\n",
    "kernel_matrix = kp_test._get_kernel_matrix(X_train_12)\n",
    "polynomial_kernel(X_train_12[0], X_train_12[0], d=3) == kernel_matrix[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took :4.896648168563843\n",
      "in-sample = % 2.39311642914762\n",
      "in-sample = % 5.483870967741935\n"
     ]
    },
    {
     "data": {
      "text/plain": "5.483870967741935"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=3)\n",
    "kpova.fit(X_train, y_train, n_epochs=1)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end - start))\n",
    "\n",
    "get_error_percentage(y_train, y_insample)\n",
    "get_error_percentage(y_test, y_outsample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *_Part 1_*\n",
    "\n",
    "### 1. *Basic results*\n",
    "- for $d=1, ... ,7$ perform 20 runs\n",
    "- split $zipcombo$ 80-20\n",
    "- report MSE and STD\n",
    "- yield a 2x7 table that has on each cell $mean+-std$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 0\n",
      "in-sample = % 8.26835170744824\n",
      "in-sample = % 8.978494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 1\n",
      "in-sample = % 8.765797257327238\n",
      "in-sample = % 10.10752688172043\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 2\n",
      "in-sample = % 13.310029577843506\n",
      "in-sample = % 14.731182795698924\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 3\n",
      "in-sample = % 9.854799677332617\n",
      "in-sample = % 11.075268817204302\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 4\n",
      "in-sample = % 10.110244689432642\n",
      "in-sample = % 10.806451612903226\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 5\n",
      "in-sample = % 10.217800484001076\n",
      "in-sample = % 11.129032258064516\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 6\n",
      "in-sample = % 8.053240118311374\n",
      "in-sample = % 9.731182795698924\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 7\n",
      "in-sample = % 11.011024468943264\n",
      "in-sample = % 11.182795698924732\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 8\n",
      "in-sample = % 15.730034955633235\n",
      "in-sample = % 15.806451612903226\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 9\n",
      "in-sample = % 8.295240656090346\n",
      "in-sample = % 9.408602150537634\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 10\n",
      "in-sample = % 9.061575692390427\n",
      "in-sample = % 10.913978494623656\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 11\n",
      "in-sample = % 8.577574616832482\n",
      "in-sample = % 9.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 12\n",
      "in-sample = % 12.436138746974994\n",
      "in-sample = % 14.56989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 13\n",
      "in-sample = % 8.712019360043023\n",
      "in-sample = % 10.376344086021506\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 14\n",
      "in-sample = % 9.317020704490455\n",
      "in-sample = % 11.559139784946236\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 15\n",
      "in-sample = % 9.653132562516806\n",
      "in-sample = % 10.053763440860216\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 16\n",
      "in-sample = % 9.989244420543157\n",
      "in-sample = % 11.397849462365592\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 17\n",
      "in-sample = % 9.088464641032536\n",
      "in-sample = % 10.86021505376344\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 18\n",
      "in-sample = % 9.357354127453616\n",
      "in-sample = % 9.56989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=1 on run 19\n",
      "in-sample = % 13.377251949448777\n",
      "in-sample = % 14.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 0\n",
      "in-sample = % 3.8047862328582953\n",
      "in-sample = % 5.752688172043011\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 1\n",
      "in-sample = % 3.5627856950793224\n",
      "in-sample = % 5.806451612903226\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 2\n",
      "in-sample = % 3.3073406829792953\n",
      "in-sample = % 5.161290322580645\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 3\n",
      "in-sample = % 3.8451196558214575\n",
      "in-sample = % 5.21505376344086\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 4\n",
      "in-sample = % 3.3342296316214037\n",
      "in-sample = % 4.516129032258065\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 5\n",
      "in-sample = % 4.006453347674106\n",
      "in-sample = % 6.182795698924731\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 6\n",
      "in-sample = % 3.0250067222371606\n",
      "in-sample = % 4.78494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 7\n",
      "in-sample = % 4.477009948910998\n",
      "in-sample = % 6.397849462365591\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 8\n",
      "in-sample = % 4.974455498789998\n",
      "in-sample = % 7.688172043010753\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 9\n",
      "in-sample = % 3.45522990051089\n",
      "in-sample = % 6.397849462365591\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 10\n",
      "in-sample = % 3.764452809895133\n",
      "in-sample = % 5.376344086021505\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 11\n",
      "in-sample = % 3.8316751815004033\n",
      "in-sample = % 6.129032258064516\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 12\n",
      "in-sample = % 3.0518956708792686\n",
      "in-sample = % 5.161290322580645\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 13\n",
      "in-sample = % 3.6165635923635384\n",
      "in-sample = % 4.78494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 14\n",
      "in-sample = % 2.82333960742135\n",
      "in-sample = % 5.376344086021505\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 15\n",
      "in-sample = % 3.9930088733530518\n",
      "in-sample = % 6.451612903225806\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 16\n",
      "in-sample = % 3.3207851573003495\n",
      "in-sample = % 4.946236559139785\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 17\n",
      "in-sample = % 3.710674912610917\n",
      "in-sample = % 5.645161290322581\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 18\n",
      "in-sample = % 2.904006453347674\n",
      "in-sample = % 4.78494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=2 on run 19\n",
      "in-sample = % 3.1997848884108633\n",
      "in-sample = % 4.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 0\n",
      "in-sample = % 1.76122613605808\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 1\n",
      "in-sample = % 1.720892713094918\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 2\n",
      "in-sample = % 2.863673030384512\n",
      "in-sample = % 5.645161290322581\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 3\n",
      "in-sample = % 2.1107824684054854\n",
      "in-sample = % 3.763440860215054\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 4\n",
      "in-sample = % 1.9763377251949448\n",
      "in-sample = % 4.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 5\n",
      "in-sample = % 2.541005646679215\n",
      "in-sample = % 4.731182795698925\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 6\n",
      "in-sample = % 2.1107824684054854\n",
      "in-sample = % 5.0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 7\n",
      "in-sample = % 2.003226673837053\n",
      "in-sample = % 5.161290322580645\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 8\n",
      "in-sample = % 1.3578919064264587\n",
      "in-sample = % 4.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 9\n",
      "in-sample = % 1.8015595590212423\n",
      "in-sample = % 3.924731182795699\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 10\n",
      "in-sample = % 2.7695617101371335\n",
      "in-sample = % 5.10752688172043\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 11\n",
      "in-sample = % 2.6216724926055393\n",
      "in-sample = % 5.860215053763441\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 12\n",
      "in-sample = % 1.9628932508738908\n",
      "in-sample = % 5.053763440860215\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 13\n",
      "in-sample = % 2.339338531863404\n",
      "in-sample = % 5.161290322580645\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 14\n",
      "in-sample = % 1.7074482387738639\n",
      "in-sample = % 4.247311827956989\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 15\n",
      "in-sample = % 1.2906695348211885\n",
      "in-sample = % 4.139784946236559\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 16\n",
      "in-sample = % 1.6402258671685936\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 17\n",
      "in-sample = % 1.9763377251949448\n",
      "in-sample = % 4.89247311827957\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 18\n",
      "in-sample = % 2.057004571121269\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=3 on run 19\n",
      "in-sample = % 1.9225598279107288\n",
      "in-sample = % 4.354838709677419\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 0\n",
      "in-sample = % 1.19655821457381\n",
      "in-sample = % 3.4408602150537635\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 1\n",
      "in-sample = % 1.6805592901317559\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 2\n",
      "in-sample = % 1.5461145469212154\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 3\n",
      "in-sample = % 1.4654477009948912\n",
      "in-sample = % 4.56989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 4\n",
      "in-sample = % 2.1242269427265392\n",
      "in-sample = % 5.32258064516129\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 5\n",
      "in-sample = % 1.156224791610648\n",
      "in-sample = % 3.010752688172043\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 6\n",
      "in-sample = % 1.747781661737026\n",
      "in-sample = % 4.516129032258065\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 7\n",
      "in-sample = % 1.1293358429685398\n",
      "in-sample = % 3.3333333333333335\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 8\n",
      "in-sample = % 1.4788921753159452\n",
      "in-sample = % 4.731182795698925\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 9\n",
      "in-sample = % 1.411669803710675\n",
      "in-sample = % 3.763440860215054\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 10\n",
      "in-sample = % 1.6133369185264856\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 11\n",
      "in-sample = % 2.016671148158107\n",
      "in-sample = % 4.67741935483871\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 12\n",
      "in-sample = % 1.1293358429685398\n",
      "in-sample = % 4.408602150537634\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 13\n",
      "in-sample = % 1.2234471632159183\n",
      "in-sample = % 4.032258064516129\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 14\n",
      "in-sample = % 1.5326700726001614\n",
      "in-sample = % 4.56989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 15\n",
      "in-sample = % 1.3041140091422425\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 16\n",
      "in-sample = % 1.8822264049475665\n",
      "in-sample = % 4.354838709677419\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 17\n",
      "in-sample = % 2.043560096800215\n",
      "in-sample = % 5.10752688172043\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 18\n",
      "in-sample = % 1.19655821457381\n",
      "in-sample = % 3.817204301075269\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=4 on run 19\n",
      "in-sample = % 1.3444474321054047\n",
      "in-sample = % 4.086021505376344\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 0\n",
      "in-sample = % 1.0755579456843238\n",
      "in-sample = % 3.924731182795699\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 1\n",
      "in-sample = % 1.4654477009948912\n",
      "in-sample = % 4.731182795698925\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 2\n",
      "in-sample = % 1.8553374563054585\n",
      "in-sample = % 4.78494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 3\n",
      "in-sample = % 1.3041140091422425\n",
      "in-sample = % 4.354838709677419\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 4\n",
      "in-sample = % 1.19655821457381\n",
      "in-sample = % 4.139784946236559\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 5\n",
      "in-sample = % 1.2368916375369723\n",
      "in-sample = % 3.010752688172043\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 6\n",
      "in-sample = % 1.169669265931702\n",
      "in-sample = % 4.516129032258065\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 7\n",
      "in-sample = % 0.9545576767948373\n",
      "in-sample = % 3.4408602150537635\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 8\n",
      "in-sample = % 1.2503361118580263\n",
      "in-sample = % 3.978494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 9\n",
      "in-sample = % 0.9948910997579995\n",
      "in-sample = % 4.623655913978495\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 10\n",
      "in-sample = % 0.9680021511158914\n",
      "in-sample = % 4.247311827956989\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 11\n",
      "in-sample = % 1.411669803710675\n",
      "in-sample = % 3.6021505376344085\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 12\n",
      "in-sample = % 0.7528905619790266\n",
      "in-sample = % 3.763440860215054\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 13\n",
      "in-sample = % 0.9545576767948373\n",
      "in-sample = % 4.354838709677419\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 14\n",
      "in-sample = % 1.3041140091422425\n",
      "in-sample = % 3.6021505376344085\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 15\n",
      "in-sample = % 2.245227211616026\n",
      "in-sample = % 4.408602150537634\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 16\n",
      "in-sample = % 1.411669803710675\n",
      "in-sample = % 3.978494623655914\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 17\n",
      "in-sample = % 1.2234471632159183\n",
      "in-sample = % 3.6559139784946235\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 18\n",
      "in-sample = % 1.2772250605001345\n",
      "in-sample = % 4.516129032258065\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=5 on run 19\n",
      "in-sample = % 0.9680021511158914\n",
      "in-sample = % 4.408602150537634\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 0\n",
      "in-sample = % 0.8066684592632428\n",
      "in-sample = % 4.731182795698925\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 1\n",
      "in-sample = % 0.887335305189567\n",
      "in-sample = % 4.139784946236559\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 2\n",
      "in-sample = % 1.3578919064264587\n",
      "in-sample = % 3.4408602150537635\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 3\n",
      "in-sample = % 0.9680021511158914\n",
      "in-sample = % 4.032258064516129\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 4\n",
      "in-sample = % 1.4923366496369992\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 5\n",
      "in-sample = % 1.6133369185264856\n",
      "in-sample = % 3.817204301075269\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 6\n",
      "in-sample = % 0.7394460876579726\n",
      "in-sample = % 4.086021505376344\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 7\n",
      "in-sample = % 1.1024468943264318\n",
      "in-sample = % 4.032258064516129\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 8\n",
      "in-sample = % 1.2637805861790803\n",
      "in-sample = % 4.56989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 9\n",
      "in-sample = % 1.1158913686474858\n",
      "in-sample = % 3.172043010752688\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 10\n",
      "in-sample = % 1.0890024200053778\n",
      "in-sample = % 3.4408602150537635\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 11\n",
      "in-sample = % 1.0621134713632696\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 12\n",
      "in-sample = % 0.9680021511158914\n",
      "in-sample = % 4.032258064516129\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 13\n",
      "in-sample = % 0.9948910997579995\n",
      "in-sample = % 4.086021505376344\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 14\n",
      "in-sample = % 1.3444474321054047\n",
      "in-sample = % 3.4946236559139785\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 15\n",
      "in-sample = % 1.6940037644528099\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 16\n",
      "in-sample = % 1.0486689970422156\n",
      "in-sample = % 3.870967741935484\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 17\n",
      "in-sample = % 1.1024468943264318\n",
      "in-sample = % 3.870967741935484\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 18\n",
      "in-sample = % 0.860446356547459\n",
      "in-sample = % 3.4946236559139785\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=6 on run 19\n",
      "in-sample = % 0.860446356547459\n",
      "in-sample = % 3.7096774193548385\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 0\n",
      "in-sample = % 1.0217800484001076\n",
      "in-sample = % 4.139784946236559\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 1\n",
      "in-sample = % 1.5864479698843774\n",
      "in-sample = % 4.301075268817204\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 2\n",
      "in-sample = % 1.1158913686474858\n",
      "in-sample = % 3.3870967741935485\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 3\n",
      "in-sample = % 1.169669265931702\n",
      "in-sample = % 4.516129032258065\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 4\n",
      "in-sample = % 1.0083355740790536\n",
      "in-sample = % 4.193548387096774\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 5\n",
      "in-sample = % 1.0755579456843238\n",
      "in-sample = % 3.763440860215054\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 6\n",
      "in-sample = % 0.8470018822264049\n",
      "in-sample = % 3.924731182795699\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 7\n",
      "in-sample = % 0.9007797795106212\n",
      "in-sample = % 2.956989247311828\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 8\n",
      "in-sample = % 0.9007797795106212\n",
      "in-sample = % 4.354838709677419\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 9\n",
      "in-sample = % 1.1158913686474858\n",
      "in-sample = % 3.924731182795699\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 10\n",
      "in-sample = % 1.0083355740790536\n",
      "in-sample = % 3.6559139784946235\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 11\n",
      "in-sample = % 1.2637805861790803\n",
      "in-sample = % 3.6559139784946235\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 12\n",
      "in-sample = % 2.1645603656897014\n",
      "in-sample = % 5.860215053763441\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 13\n",
      "in-sample = % 1.183113740252756\n",
      "in-sample = % 4.086021505376344\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 14\n",
      "in-sample = % 0.9680021511158914\n",
      "in-sample = % 3.5483870967741935\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 15\n",
      "in-sample = % 1.411669803710675\n",
      "in-sample = % 4.623655913978495\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 16\n",
      "in-sample = % 0.8201129335842968\n",
      "in-sample = % 3.225806451612903\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 17\n",
      "in-sample = % 1.1293358429685398\n",
      "in-sample = % 3.3870967741935485\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 18\n",
      "in-sample = % 1.2100026888948643\n",
      "in-sample = % 3.7096774193548385\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "d=7 on run 19\n",
      "in-sample = % 1.0217800484001076\n",
      "in-sample = % 3.4408602150537635\n",
      "took 682.85400557518\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "means_train = []\n",
    "means_test = []\n",
    "stds_train = []\n",
    "stds_test = []\n",
    "\n",
    "start=time.time()\n",
    "for d in range(1, 8):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for run in range(20):\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=d)\n",
    "\n",
    "        #train\n",
    "        kpova.fit(X_train, y_train, n_epochs=1)\n",
    "        y_insample = kpova.predict(X_train)\n",
    "        #test\n",
    "        y_outsample = kpova.predict(X_test)\n",
    "\n",
    "        print('\\nd=' + str(d) + ' on run ' + str(run))\n",
    "        train_errors.append(get_error_percentage(y_train, y_insample))\n",
    "        test_errors.append(get_error_percentage(y_test, y_outsample))\n",
    "    means_train.append(np.mean(train_errors))\n",
    "    means_test.append(np.mean(test_errors))\n",
    "    stds_train.append(np.std(train_errors))\n",
    "    stds_test.append(np.std(test_errors))\n",
    "end = time.time()\n",
    "\n",
    "print('took {t}'.format(t=end-start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_1a = pd.DataFrame({\n",
    "    'mean train': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_train, stds_train)],\n",
    "    'mean test': [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(means_test, stds_test)],\n",
    "})\n",
    "\n",
    "display(table_1a)\n",
    "table_1a.style.to_latex('table_1a.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N_epochs_tuning: _But firstly we need to find how many epochs to train_\n",
    "#### *i.e. A discussion of any parameters of your method which were not cross-validated over.*\n",
    "\n",
    "I ran the basic results algorithm on all $d$s only with 1 epoch in order to observe the \"best\" $d$ for 1 epoch on this dataset and get an intuition.\n",
    "\n",
    "Now I will create a graph that shows the errors for each number of epochs, setting $d=[5, 7]$, plotting the average errors observed per epoch.\n",
    "In order to offset the risk of being biased on a specific order of training datapoints, we will shuffle and split the dataset 20 times and average those per epoch. Each time an epoch analysis(1-15), a new classifier is defined for each nr of epochs, to show the progress individually.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D = 5\n",
      "\n",
      "   RUN 0\n",
      "Train data set size = 7438\n",
      "Test data set size = 1860\n",
      "\n",
      "           EPOCH 1\n",
      "in-sample = % 1.4923366496369992\n",
      "in-sample = % 3.817204301075269\n",
      "\n",
      "           EPOCH 2\n",
      "in-sample = % 0.5243344985211078\n",
      "in-sample = % 3.3870967741935485\n",
      "\n",
      "           EPOCH 3\n",
      "in-sample = % 0.26888948642108096\n",
      "in-sample = % 2.903225806451613\n",
      "\n",
      "           EPOCH 4\n",
      "in-sample = % 0.1478892175315945\n",
      "in-sample = % 2.849462365591398\n",
      "\n",
      "           EPOCH 5\n",
      "in-sample = % 0.04033342296316214\n",
      "in-sample = % 2.795698924731183\n",
      "\n",
      "           EPOCH 6\n",
      "in-sample = % 0.026888948642108095\n",
      "in-sample = % 2.6344086021505375\n",
      "\n",
      "           EPOCH 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#train\u001B[39;00m\n\u001B[0;32m     18\u001B[0m kpova\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, n_epochs\u001B[38;5;241m=\u001B[39mn_epochs)\n\u001B[1;32m---> 19\u001B[0m y_insample \u001B[38;5;241m=\u001B[39m \u001B[43mkpova\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m#test\u001B[39;00m\n\u001B[0;32m     21\u001B[0m y_outsample \u001B[38;5;241m=\u001B[39m kpova\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "Cell \u001B[1;32mIn[15], line 96\u001B[0m, in \u001B[0;36mKPOneVsAllClassifier.predict\u001B[1;34m(self, X_test)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X_test: np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;66;03m## for polynomial kernel\u001B[39;00m\n\u001B[1;32m---> 96\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mK_test \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpower\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mAlpha \u001B[38;5;241m@\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mK_test), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "errors_per_d = []\n",
    "\n",
    "for d in range(5, 8):\n",
    "    print('\\nD = {d}'.format(d=d))\n",
    "    epoch_train_errors = np.array(np.zeros((20, 15))) # size 20, 15\n",
    "    epoch_test_errors = np.array(np.zeros((20, 15))) # size 20, 15\n",
    "\n",
    "    for run in range(20):\n",
    "        print('\\n   RUN {r}'.format(r=run))\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        train_errors = np.array(np.zeros(15)) # size 15\n",
    "        test_errors = np.array(np.zeros(15)) #size 15\n",
    "        for n_epochs in range(1, 15):\n",
    "            print('\\n           EPOCH {e}'.format(e=n_epochs))\n",
    "            kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=d)\n",
    "            #train\n",
    "            kpova.fit(X_train, y_train, n_epochs=n_epochs)\n",
    "            y_insample = kpova.predict(X_train)\n",
    "            #test\n",
    "            y_outsample = kpova.predict(X_test)\n",
    "\n",
    "            train_errors[n_epochs-1] = (get_error_percentage(y_train, y_insample))\n",
    "            test_errors[n_epochs-1] = (get_error_percentage(y_test, y_outsample))\n",
    "        epoch_train_errors[run] = train_errors\n",
    "        epoch_test_errors[run] = test_errors\n",
    "    # take the mean of 20 runs for every n_epoch = per columns\n",
    "    errors_per_d.append((epoch_train_errors.mean(axis=0), epoch_test_errors.mean(axis=0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for errors_d in errors_per_d:\n",
    "    train_errors, test_errors = errors_d\n",
    "\n",
    "    x = [epoch for epoch in range(1, 15)]\n",
    "    print(x.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.title('Errors per epoch D={d}'.format)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error%')\n",
    "    plt.plot(x, train_errors, color = \"red\", linewidth = 1.5, linestyle = \"-.\", label = \"train errors\")\n",
    "    plt.plot(x, test_errors, marker = '+', linestyle = '-', label = 'test_errors')\n",
    "    plt.legend(loc = \"upper left\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. _Cross validation_\n",
    "- perform 20 runs\n",
    "- shuffle and split data 80-20\n",
    "- select \"best\" parameter $d*$ using 5-fold cross-validation on the 80% training data\n",
    "- retrain on full 80% training using $d*$\n",
    "- record the test errors for 20%\n",
    "- findings: 20 $d*$ and 20 test errors\n",
    "- output: mean_test_error$\\pm$std, mean_$d*\\pm$std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def cross_validation(X, y, n_folds):\n",
    "    fold_size = X.shape[0] // n_folds\n",
    "\n",
    "    split_idxs = [i * fold_size - 1 for i in range(1, 5)]\n",
    "    X_folds_list = np.split(X, indices_or_sections=split_idxs)\n",
    "    y_folds_list = np.split(y, indices_or_sections=split_idxs)\n",
    "    assert len(X_folds_list) == len(y_folds_list) == n_folds\n",
    "\n",
    "    errors_d = []\n",
    "    for d in range(1, 3):\n",
    "        local_errors = []\n",
    "        for i in range(n_folds):\n",
    "            # Create a training and test folds from given data\n",
    "            X_train_fold = np.vstack(([X_folds_list[k] for k in range(0, n_folds) if k != i]))\n",
    "            y_train_fold = np.concatenate([y_folds_list[k] for k in range(0, n_folds) if k != i], axis=0)\n",
    "            X_validation_fold = X_folds_list[i]\n",
    "            y_validation_fold = y_folds_list[i]\n",
    "\n",
    "            kp = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=d)\n",
    "            kp.fit(X_train_fold, y_train_fold, n_epochs=7)\n",
    "            y_preds = kp.predict(X_validation_fold)\n",
    "            local_errors.append(get_error_percentage(y_validation_fold, y_preds))\n",
    "\n",
    "        errors_d.append(local_errors)\n",
    "\n",
    "    # errors[index + 1] = d, where d==index\n",
    "    np.argmax(np.mean(errors_d, axis=1))\n",
    "    return best_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. _Confusion matrix_\n",
    "\n",
    "- choosing the best $d*$ and retrain on the full 80%\n",
    "- produce a confusion matrix 10x10 that records error rates per digit as following:\n",
    "    - (7,2) means: ground truth $y$ is 7 but 2 was predicted($y_hat$)\n",
    "    - error rate calculates as following: nr of mistakes for that digit / nr of instances of that digit\n",
    "- each cell will have 20 outcomes, since there are 20 runs\n",
    "- average them and get the std dev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1553, 1269,  929,  824,  852,  716,  834,  792,  708,  821])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of instances of each digit in the dataset\n",
    "# since we just reshuffle, the number holds for cross validation as well\n",
    "instances = np.array(np.zeros(10, dtype='int'))\n",
    "\n",
    "for y in y_train:\n",
    "    instances[int(y)]+=1\n",
    "for y in y_test:\n",
    "    instances[int(y)]+=1\n",
    "\n",
    "instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cross_validation(X, y, n_folds):\n",
    "    fold_size = X.shape[0] // n_folds\n",
    "\n",
    "    split_idxs = [i * fold_size - 1 for i in range(1, 5)]\n",
    "    X_folds_list = np.split(X, indices_or_sections=split_idxs)\n",
    "    y_folds_list = np.split(y, indices_or_sections=split_idxs)\n",
    "    assert len(X_folds_list) == len(y_folds_list) == n_folds\n",
    "\n",
    "    errors_d = []\n",
    "    for d in range(1, 3):\n",
    "        local_errors = []\n",
    "        for i in range(n_folds):\n",
    "            # Create a training and test folds from given data\n",
    "            X_train_fold = np.vstack(([X_folds_list[k] for k in range(0, n_folds) if k != i]))\n",
    "            y_train_fold = np.concatenate([y_folds_list[k] for k in range(0, n_folds) if k != i], axis=0)\n",
    "            X_validation_fold = X_folds_list[i]\n",
    "            y_validation_fold = y_folds_list[i]\n",
    "\n",
    "            kp = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=d)\n",
    "            kp.fit(X_train_fold, y_train_fold, n_epochs=7)\n",
    "            y_preds = kp.predict(X_validation_fold)\n",
    "            local_errors.append(get_error_percentage(y_validation_fold, y_preds))\n",
    "\n",
    "        errors_d.append(local_errors)\n",
    "\n",
    "    # errors[index + 1] = d, where d==index\n",
    "    np.argmax(np.mean(errors_d, axis=1))\n",
    "    return best_d\n",
    "\n",
    "#### DELETE AFTER DEBUG\n",
    "\n",
    "best_ds = []\n",
    "test_errors = []\n",
    "confusion_matrix = np.array(np.zeros((10, 10, 20)))\n",
    "\n",
    "for run in range(2):\n",
    "    shuffled = np.random.permutation(data)\n",
    "    train_data, test_data = split_80_20(shuffled)\n",
    "    X_train, y_train = split_X_y(train_data)\n",
    "    X_test, y_test = split_X_y(test_data)\n",
    "\n",
    "    best_d = cross_validation(X_train, y_train, 5)\n",
    "\n",
    "    kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=best_d)\n",
    "    kpova.fit(X_train, y_train, n_epochs=7)\n",
    "    y_outsample = kpova.predict(X_test)\n",
    "    test_error = get_error_percentage(y_test, y_outsample)\n",
    "\n",
    "    # get errors per digit for CONFUSION MATRIX\n",
    "    local_conf = np.array(np.zeros((10, 10)))\n",
    "    for (truth, pred) in zip(y_test, y_outsample):\n",
    "        if truth!=pred:\n",
    "            local_conf[int(truth), int(pred)] += 1\n",
    "\n",
    "    # in local_conf divide each row by instances[row]\n",
    "    # stack the entire matrix in the third dimension\n",
    "    confusion_matrix[:, :, run] = local_conf / instances[:, None]\n",
    "\n",
    "    best_ds.append(best_d)\n",
    "    test_errors.append(test_error)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# point 2 - CROSS VALIDATION\n",
    "ds_errors = pd.DataFrame({\n",
    "    'best_ds' : [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(np.mean(best_ds), np.std(best_ds))],\n",
    "    'test_errors' : [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(np.mean(test_errors), np.std(test_errors))]\n",
    "})\n",
    "\n",
    "ds_errors.to_csv('best_d_errors.csv')\n",
    "ds_errors.to_latex('best_d_errors.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# point 3 - CONFUSION MATRIX\n",
    "\n",
    "mean_confusion = confusion_matrix.mean(axis=2)\n",
    "std_confusion = confusion_matrix.std(axis=2)\n",
    "mean_std = np.array(np.zeros((10, 10))).astype('U')\n",
    "\n",
    "for i in range(mean_confusion.shape[0]):\n",
    "    mean_std[i] = [str(f'{x:.3f}') + u\"\\u00B1\" + str(f'{y:.3f}') for (x, y) in zip(mean_confusion[i], std_confusion[i])]\n",
    "\n",
    "mean_std_df = pd.DataFrame(mean_std)\n",
    "mean_std_df.to_csv('confusion_matrix.csv')\n",
    "mean_std_df.to_latex('confusion_matrix.tex')\n",
    "\n",
    "mean_std_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# create confusion matrix heatmap\n",
    "sns.heatmap(mean_confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}