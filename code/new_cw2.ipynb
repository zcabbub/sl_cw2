{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing all modules in cell below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "data = np.loadtxt('../support/zipcombo.dat')\n",
    "# data = np.loadtxt('../support/dtrain123.dat')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(9298, 257)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define functions for splitting the dataset into train/test and input/label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def split_80_20(data: np.ndarray, seed: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits 80% train 20% test\n",
    "\n",
    "    :param data: sequence.\n",
    "    :param seed: Random seed used to shuffle given data.\n",
    "    :return: train_data, test_data: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = data.shape[0]\n",
    "    shuffle = np.random.permutation(data)\n",
    "    train_size = int(n*0.8)\n",
    "    return shuffle[:train_size], shuffle[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def split_X_y(data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Splits the data into datapoints and labels, X_train matrix and y_train;\n",
    "    :param data: np.ndarray\n",
    "    :return: X_train, y_train: np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, 1:], data[:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def shuffle_split(data):\n",
    "    data_train, data_test = split_80_20(data, seed=637)\n",
    "    X_train, y_train = split_X_y(data_train)\n",
    "    X_test, y_test = split_X_y(data_test)\n",
    "\n",
    "    assert X_train.shape[0] == y_train.size\n",
    "    assert X_test.shape[0] == y_test.size\n",
    "\n",
    "    print(\"Train data set size = %d\" % X_train.shape[0])\n",
    "    print(\"Test data set size = %d\" % X_test.shape[0])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size = 7438\n",
      "Test data set size = 1860\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = shuffle_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def display_digit(grayscale):\n",
    "    plt.imshow(np.reshape(grayscale, (16, 16)), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(X_train[100])\n",
    "print(y_train[100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now define the Kernel Perceptron algorithm and kernel functions to be used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def polynomial_kernel(x_i: np.ndarray, x_t: np.ndarray, d: int):\n",
    "    return np.inner(x_i, x_t) ** d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def gaussian_kernel(x_i: np.ndarray, x_j: np.ndarray, sigma=1):\n",
    "    if x_i.shape[0] != x_j.shape[0]:\n",
    "        raise Exception(\"Cannot apply kernel to vectors of different dimensions: x_i has shape {s1}, x_j has shape {s2}\"\n",
    "                        .format(s1=x_i.shape, s2=x_j.shape))\n",
    "    diff = x_i - x_j\n",
    "    return np.exp(-1 * np.inner(diff, diff) / (2 * sigma**2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class KernelPerceptron:\n",
    "    def __init__(self, kernel: Callable[[np.ndarray, np.ndarray], float], size: int):\n",
    "        self.kernel = kernel\n",
    "        self.K = None\n",
    "        self.alpha = np.zeros(size)\n",
    "        self.X_train = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "\n",
    "        for i in range(0, X_train.shape[0]):\n",
    "            for j in range(i, X_train.shape[0]):\n",
    "                K[i, j] = K[j, i] = self.kernel(X_train[i], X_train[j])\n",
    "\n",
    "        return K\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray, n_epochs: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prediction function that populates the alpha parameter, 1 data point at a time;\n",
    "        :param X_train: training data points\n",
    "        :param y_train: training corresponding labels\n",
    "        :param n_epochs: number of times to pass through the data. Alpha contains the learning parameters that get inherited from one epoch to the other\n",
    "        :return: y_preds: the predictions enhanced after N epochs\n",
    "\n",
    "        \"\"\"\n",
    "        if y_train.ndim != 1:\n",
    "            raise Exception('y_train must be a 1-dim np.ndarray. Given y_train with shape {s}'.format(s=y_train.shape))\n",
    "        if X_train.shape[0] != y_train.size:\n",
    "            raise Exception('X_train and y_train must contain equal number of samples. Given X_train with shape {s1} and y_train with shape {s2}'.format(s1=y_train.shape, s2=X_train.shape))\n",
    "\n",
    "        y_preds = np.zeros(X_train.shape[0])\n",
    "        self.X_train = X_train\n",
    "\n",
    "        # Compute kernel matrix which stays constant throughout the algorithm and all epochs\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "\n",
    "        for epoch in range(0, n_epochs):\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                y_pred = self._predict_single(t)\n",
    "                y_preds[t] = y_pred\n",
    "                if y_pred != y_train[t]:\n",
    "                    self.alpha[t] += y_train[t]\n",
    "        return y_preds\n",
    "\n",
    "    def _predict_single(self, t: int):\n",
    "        \"\"\"\n",
    "        We take a whole row of kernel matrix K because in we want to account for errors in previous epochs.\n",
    "        This is not an issue in the first epoch because all alpha's > t are 0.\n",
    "        :param t: iteration step in the online learning algorithm.\n",
    "        :return: y hat, single predicted value at step t.\n",
    "        \"\"\"\n",
    "        return np.sign(np.inner(self.K[t, :], self.alpha))\n",
    "\n",
    "    def yhat_single(self, K, t: int):\n",
    "        return np.sign(np.inner(K[t, :], self.alpha))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prediction function to be used for out-of-sample test data point.\n",
    "        Does not perform online learning (update step).\n",
    "        :param X: test data points.\n",
    "        :return: np.ndarray of predictions for each given test data point.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x_test in X:\n",
    "            sum = 0\n",
    "            for t in range(self.X_train.shape[0]):\n",
    "                # Can improve by using kernel matrix\n",
    "                sum += self.alpha[t] * self.kernel(self.X_train[t], x_test)\n",
    "            predictions.append(np.sign(sum))\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# kp = KernelPerceptron(kernel=partial(polynomial_kernel, d=3))\n",
    "# y_insample = kp.fit(X_train_12, y_train_12, n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took :21.00102186203003\n",
      "in-sample = % 0.06722237160527024\n",
      "out-of-sample = % 2.849462365591398\n"
     ]
    }
   ],
   "source": [
    "def get_error_percentage(y_train, y_insample, y_test, y_outsample):\n",
    "    print(\"in-sample = % \" + str(100 * get_num_mistakes(actual=y_train, predicted=y_insample) / y_train.size))\n",
    "    print(\"out-of-sample = % \" + str(100 * get_num_mistakes(actual=y_test, predicted=y_outsample) / y_test.size))\n",
    "\n",
    "def get_num_mistakes(actual: np.ndarray, predicted: np.ndarray) -> int:\n",
    "    # or calculating by checking which alpha values are different than 0? alpha is 0 when the prediction matches\n",
    "    diffs = actual - predicted\n",
    "    n_mistakes = 0\n",
    "    for diff in diffs:\n",
    "        if diff != 0:\n",
    "            n_mistakes += 1\n",
    "    return n_mistakes\n",
    "\n",
    "class KPOneVsAllClassifier():\n",
    "    def __init__(self, kernel, n_classes, d):\n",
    "        self.kernel = kernel\n",
    "        self.n_classes = n_classes # could optimize to remove this var since =k.shape[0]\n",
    "        self.d = d\n",
    "        self.X_train = None\n",
    "        self.Alpha = None\n",
    "        self.K = None\n",
    "\n",
    "    def _get_kernel_matrix(self, X_train: np.ndarray) -> np.ndarray:\n",
    "        ### TODO: Takes a lot of time, we should improve efficiency\n",
    "        # K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "        # for i in range(0, X_train.shape[0]):\n",
    "        #     for j in range(i, X_train.shape[0]):\n",
    "        #         K[i, j] = K[j, i] = self.kernel(X_train[i], X_train[j], d=self.d)\n",
    "        # return K\n",
    "        return np.power(X_train @ X_train.T, self.d)\n",
    "\n",
    "    def sign(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x <= 0, -1, 1)\n",
    "\n",
    "    def _predict_single_confidence(self, t, y_train):\n",
    "        # Get prediction array P\n",
    "        # preds = (self.Alpha @ self.K)[:, t]\n",
    "        preds = self.Alpha @ self.K[t]\n",
    "\n",
    "        # Get Y_t array of ground truth (duplicate y_t)\n",
    "        y = np.full(self.n_classes, -1)\n",
    "        y[int(y_train[t])] = 1\n",
    "\n",
    "        # penilize\n",
    "        # how should the alpha look like on miss on y=7 ? 1 1 1 1 1 1 1 -1 1 1 (only 7 is penilized?) or -1 -1 ... -1 (everybody is penilized\n",
    "        # should we only penilize the respective perceptron? e.g., if y=7, and perceptron[7]=0, should we penilize just perceptron 7?\n",
    "        self.Alpha[:, t] -= np.heaviside(-(preds * y), 1) * self.sign(preds)\n",
    "        return preds\n",
    "        # for each perceptron\n",
    "        # for cl in range(self.n_classes):\n",
    "        #     # create and save predictions\n",
    "        #     pred = np.inner(self.K[t, :], perceptrons[cl].alpha)\n",
    "        #     confidence[cl] = pred\n",
    "        #\n",
    "        #     y = 1 if y_train[t] == float(cl) else (-1)\n",
    "        #     ## penalize if the prediction wrong\n",
    "        #     if y*pred <= 0:\n",
    "        #         perceptrons[cl].alpha[t] -= self.sign(pred)\n",
    "\n",
    "    def fit(self, X_train, y_train, n_epochs):\n",
    "        y_preds = np.array(np.zeros(X_train.shape[0]))\n",
    "\n",
    "        self.K = self._get_kernel_matrix(X_train)\n",
    "        self.Alpha = np.zeros((self.n_classes, X_train.shape[0]))\n",
    "        self.X_train = X_train\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # for each point, calculate confidence and make predictions\n",
    "            for t in range(0, X_train.shape[0]):\n",
    "                confidence = self._predict_single_confidence(t, y_train)\n",
    "                # the index with the highest number(confidence) is the prediction\n",
    "                # +1 because the index for confidence in perceptron[1] is 0\n",
    "                y_preds[t] = np.argmax(confidence) + 1\n",
    "        return y_preds\n",
    "\n",
    "    # def _choose_best_ytest(self, yhats):\n",
    "    #     # get the classifiers that predicted positive\n",
    "    #     maxims = np.argwhere(yhats == np.amax(yhats)).flatten() + 1\n",
    "    #     if maxims.shape[0] == 1:\n",
    "    #         # if only 1 classifier predicts positive, take that class\n",
    "    #          return maxims[0]\n",
    "    #     else:\n",
    "    #         # if more than one predicted positive, or all of them negative, choose one randomly\n",
    "    #         return np.random.choice(maxims)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        ## for polynomial kernel\n",
    "        self.K_test = np.power(X_train @ X_test.T, self.d)\n",
    "        return np.argmax((self.Alpha @ self.K_test), axis=0)\n",
    "        #\n",
    "        # for t_test in range(X_test.shape[0]):\n",
    "        #     yhats_sums = self.Alpha @ self.K_test[t_test]\n",
    "        #     ypreds[t_test] = self._choose_best_ytest(yhats_sums)\n",
    "        #     yhats_sums = np.array(np.zeros(self.n_classes))\n",
    "        #     for cl in range(self.n_classes):\n",
    "        #         # update\n",
    "        #         sum = np.inner(self.Alpha[:, t])\n",
    "        #         for t in range(self.X_train.shape[0]):\n",
    "        #             sum += self.perceptrons[cl].alpha[t] * self.kernel(self.X_train[t], X_test[t_test], self.d)\n",
    "        #         yhats_sums[cl] = sum\n",
    "        # return ypreds\n",
    "\n",
    "\n",
    "### for in-cell debug\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=3)\n",
    "kpova.fit(X_train, y_train, n_epochs=50)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end-start))\n",
    "\n",
    "get_error_percentage(y_train, y_insample, y_test, y_outsample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (7438,) and (10,) not aligned: 7438 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      5\u001B[0m kpova \u001B[38;5;241m=\u001B[39m KPOneVsAllClassifier(polynomial_kernel, n_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, d\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m \u001B[43mkpova\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m y_insample \u001B[38;5;241m=\u001B[39m kpova\u001B[38;5;241m.\u001B[39mpredict(X_train)\n\u001B[0;32m      8\u001B[0m y_outsample \u001B[38;5;241m=\u001B[39m kpova\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "Cell \u001B[1;32mIn[35], line 67\u001B[0m, in \u001B[0;36mKPOneVsAllClassifier.fit\u001B[1;34m(self, X_train, y_train, n_epochs)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# for each point, calculate confidence and make predictions\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m---> 67\u001B[0m         confidence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict_single_confidence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m         \u001B[38;5;66;03m# the index with the highest number(confidence) is the prediction\u001B[39;00m\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;66;03m# +1 because the index for confidence in perceptron[1] is 0\u001B[39;00m\n\u001B[0;32m     70\u001B[0m         y_preds[t] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(confidence) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[35], line 37\u001B[0m, in \u001B[0;36mKPOneVsAllClassifier._predict_single_confidence\u001B[1;34m(self, t, y_train)\u001B[0m\n\u001B[0;32m     34\u001B[0m confidence \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes))\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Get prediction array P\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mK\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAlpha\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Get Y_t array of ground truth (duplicate y_t)\u001B[39;00m\n\u001B[0;32m     40\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfull(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36minner\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (7438,) and (10,) not aligned: 7438 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=3)\n",
    "kpova.fit(X_train, y_train, n_epochs=1)\n",
    "y_insample = kpova.predict(X_train)\n",
    "y_outsample = kpova.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"took :{t}\".format(t=end-start))\n",
    "\n",
    "get_error_percentage(y_train, y_insample, y_test, y_outsample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Tests_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us test the Kernel Perceptron implementation on a dummy dataset of only digits 1 and 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Filter out digit 3 (leave only digit 1 and 2) from dtrain123.dat train and test dataset\n",
    "indxs_digit_3 = np.where(y_train_123 == 3)\n",
    "X_train_12 = np.delete(X_train_123, indxs_digit_3, axis=0)\n",
    "y_train_12 = np.delete(y_train_123, indxs_digit_3)\n",
    "y_train_12[y_train_12 == 1] = -1\n",
    "y_train_12[y_train_12 == 2] = 1\n",
    "\n",
    "indxs_digit_3 = np.where(y_test_123 == 3)\n",
    "X_test_12 = np.delete(X_test_123, indxs_digit_3, axis=0)\n",
    "y_test_12 = np.delete(y_test_123, indxs_digit_3)\n",
    "y_test_12[y_test_12 == 1] = -1\n",
    "y_test_12[y_test_12 == 2] = 1\n",
    "\n",
    "assert X_train_12.shape[0] == y_train_12.size\n",
    "assert X_test_12.shape[0] == y_test_12.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample = % 0.0\n",
      "out-of-sample = % 1.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "# Test fitting and in-sample predictions\n",
    "kp = KernelPerceptron(kernel=partial(polynomial_kernel, d=5), size=X_train_12.shape[0])\n",
    "y_insample = kp.fit(X_train_12, y_train_12, n_epochs=2)\n",
    "# Test out-of-sample predictions\n",
    "y_outsample = kp.predict(X_test_12) # all predictions of test sample in y_outsample\n",
    "print(\"in-sample = % \" + str(100 * get_num_mistakes(actual=y_train_12, predicted=y_insample) / y_train_12.size))\n",
    "print(\"out-of-sample = % \" + str(100 * get_num_mistakes(actual=y_test_12, predicted=y_outsample) / y_test_12.size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KernelPerceptron.__init__() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m## Test the kernel matrix\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m kp_test \u001B[38;5;241m=\u001B[39m \u001B[43mKernelPerceptron\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolynomial_kernel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m kernel_matrix \u001B[38;5;241m=\u001B[39m kp_test\u001B[38;5;241m.\u001B[39m_get_kernel_matrix(X_train_12)\n\u001B[0;32m      4\u001B[0m polynomial_kernel(X_train_12[\u001B[38;5;241m0\u001B[39m], X_train_12[\u001B[38;5;241m0\u001B[39m], d\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m) \u001B[38;5;241m==\u001B[39m kernel_matrix[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: KernelPerceptron.__init__() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": [
    "## Test the kernel matrix\n",
    "kp_test = KernelPerceptron(kernel=partial(polynomial_kernel, d=3))\n",
    "kernel_matrix = kp_test._get_kernel_matrix(X_train_12)\n",
    "polynomial_kernel(X_train_12[0], X_train_12[0], d=3) == kernel_matrix[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *_Part 1_*\n",
    "\n",
    "1. *Basic results*\n",
    "- for $d=1, ... ,7$ perform 20 runs\n",
    "- split $zipcombo$ 80-20\n",
    "- report MSE and STD\n",
    "- yield a 2x7 table that has on each cell $mean+-std$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means_train = []\n",
    "means_test = []\n",
    "stds_train = []\n",
    "stds_test = []\n",
    "\n",
    "for d in range(1, 8):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for run in range(20):\n",
    "        X_train, y_train, X_test, y_test = shuffle_split(data)\n",
    "\n",
    "        kpova = KPOneVsAllClassifier(polynomial_kernel, n_classes=10, d=d)\n",
    "\n",
    "        #train\n",
    "        kpova.fit(X_train, y_train, n_epochs=1)\n",
    "        y_insample = kpova.predict(X_train)\n",
    "        #test\n",
    "        y_outsample = kpova.predict(X_test)\n",
    "\n",
    "        train_errors.append(get_error_percentage(y_train, y_insample))\n",
    "        test_errors.append(get_error_percentage(y_test, y_outsample))\n",
    "    means_train.append(np.mean(train_errors))\n",
    "    means_test.append(np.mean(test_errors))\n",
    "    stds_train.append(np.std(train_errors))\n",
    "    stds_test.append(np.std(test_errors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame({\n",
    "    'mean train': [str(x) + u\"\\u00B1\" + str(y) for (x, y) in zip(means_train, stds_train)],\n",
    "    'mean test': [str(x) + u\"\\u00B1\" + str(y) for (x, y) in zip(means_test, stds_test)],\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. _Cross validation_\n",
    "- perform 20 runs\n",
    "- select \"best\" parameter $d*$ using 5-fold cross-validation\n",
    "- retrain on 80% training using $d*$ and record the test errors for 20%\n",
    "- findings: 20 $d*$ and 20 test errors\n",
    "- output: mean_test_error$\\pm$std, mean_$d*\\pm$std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "possible optimizations:\n",
    "- in predict_single_confidence, try to fill the confidence vector for all perceptrons in 1 numpy operation\n",
    "- in predict_single_confidence, try to compose the y*pred with an np.inner and then go and update the alphas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}